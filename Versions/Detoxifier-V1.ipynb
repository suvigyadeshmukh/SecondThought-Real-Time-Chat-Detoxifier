{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "445d4928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lightweight AI model (all-MiniLM-L6-v2)...\n",
      "System Ready. Context-aware model loaded.\n",
      "\n",
      "Test 1: {'original_text': 'You are so stupid and useless', 'correct_text': 'I just ignore you', 'toxicity': 0.8059, 'severity': 'MODERATE', 'latencyinms': 48.87}\n",
      "\n",
      "Test 2: {'original_text': 'shut your mouth', 'correct_text': 'you have no strength', 'toxicity': 0.4799, 'severity': 'MODERATE', 'latencyinms': 33.57}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#provided configuration(drop down list)\n",
    "PLATFORM_CONFIG = {\n",
    "    \"gaming\": {\"threshold\": 0.6},  # Gamers talk trash; higher tolerance\n",
    "    \"social\": {\"threshold\": 0.5},   # Default\n",
    "    \"professional\": {\"threshold\": 0.3}, # Strict\n",
    "    \"kids\": {\"threshold\": 0.15}    # Very Strict\n",
    "}\n",
    "\n",
    "# Limits(Constants)\n",
    "SAFE = 0.4\n",
    "UNSAFE = 0.85 \n",
    "\n",
    "class DetoxifySmart:\n",
    "    def __init__(self, config=None):\n",
    "        #making default config(other values are updated in filter api section only suggestion one is made default here)\n",
    "        self.config = {\n",
    "            \"enable_suggestions\": True\n",
    "        }\n",
    "        if config:\n",
    "            self.config.update(config)\n",
    "\n",
    "        print(\"Loading lightweight AI model (all-MiniLM-L6-v2)...\")\n",
    "        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        self.model = LogisticRegression(max_iter=1000)\n",
    "        \n",
    "        self.df = None\n",
    "        self.toxic_embeddings = None\n",
    "\n",
    "    # xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Training Portion xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "    def load_and_train(self):\n",
    "        url = \"https://raw.githubusercontent.com/s-nlp/paradetox/main/paradetox/paradetox.tsv\"\n",
    "        data = pd.read_csv(url, sep=\"\\t\")\n",
    "        \n",
    "        #training data\n",
    "        toxic_samples = data.iloc[:, 0].head(4000)\n",
    "        safe_samples = data.iloc[:, 1].head(4000)\n",
    "        \n",
    "        #Generate Embeddings (happens once at startup)\n",
    "        # converts text to 384-dimensional meaningful vectors\n",
    "        X_toxic = self.embedder.encode(toxic_samples)\n",
    "        X_safe = self.embedder.encode(safe_samples)\n",
    "        \n",
    "        X = np.vstack([X_toxic, X_safe])\n",
    "        \n",
    "        part1 = np.full(len(toxic_samples), 1)\n",
    "        part2 = np.full(len(safe_samples), 0)\n",
    "        y = np.concatenate([part1, part2])\n",
    "\n",
    "        #training the model\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "        #xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Sugesstion Part xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "        if self.config[\"enable_suggestions\"]:\n",
    "            self.df = pd.DataFrame({\n",
    "                \"toxic\": toxic_samples,\n",
    "                \"suggestion\": safe_samples\n",
    "            })\n",
    "            self.toxic_embeddings = X_toxic # Store pre-computed vectors\n",
    "            \n",
    "        print(\"System Ready. Context-aware model loaded.\")\n",
    "\n",
    "    # xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx FILTER API xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "    #payload is ur json dictionary input\n",
    "    def filter_text(self, payload):\n",
    "        \n",
    "        #start time -> used at end again to see time elapsed\n",
    "        start = time.time()\n",
    "        \n",
    "        #extracting and setting the defaults (need,default)\n",
    "        text = payload.get(\"text\", \"\")\n",
    "        platform = payload.get(\"platform\", \"social\")\n",
    "        \n",
    "        #currently unnecessary( incase in future we add more configuration ignoring the threshold)\n",
    "        current_threshold = PLATFORM_CONFIG.get(platform, {}).get(\"threshold\", 0.5)\n",
    "\n",
    "        # taking input\n",
    "        input = self.embedder.encode([text])\n",
    "        \n",
    "        #Predict Toxicity\n",
    "        # predict_proba returns [ProbabilitySafe, ProbabilityToxic]\n",
    "        score = float(self.model.predict_proba(input)[0][1])\n",
    "\n",
    "        #chat changing\n",
    "        apireturn = text\n",
    "        severity = \"SAFE\"\n",
    "        \n",
    "        if score >= UNSAFE:\n",
    "            severity = \"SEVERE\"\n",
    "            apireturn = \"* [Censored] *\"\n",
    "            \n",
    "        elif score >= current_threshold:\n",
    "            severity = \"MODERATE\"\n",
    "            \n",
    "            if self.config[\"enable_suggestions\"]:\n",
    "                \n",
    "                # Semantic Search => finds the toxic sentence in our DB that is closest in meaning to the user's input\n",
    "                \n",
    "                sims = cosine_similarity(input, self.toxic_embeddings)\n",
    "                index_similiar = np.argmax(sims)\n",
    "                similarity_score = sims[0][index_similiar]\n",
    "                \n",
    "                # Only suggest if we are somewhat confident (similarity > 0.4)\n",
    "                if similarity_score > 0.4:\n",
    "                    apireturn = self.df.iloc[index_similiar][\"suggestion\"]\n",
    "                else:\n",
    "                    # If we catch toxicity but don't have a good suggestion\n",
    "                    apireturn = \"[Comment flagged for toxicity]\"\n",
    "            else:\n",
    "                apireturn = \"[Hidden]\"\n",
    "\n",
    "        #Latency(time final - time inital[when filter function is called])\n",
    "        latency = round((time.time() - start) * 1000, 2)\n",
    "\n",
    "        return {\n",
    "            \"original_text\": text,\n",
    "            \"correct_text\": apireturn,\n",
    "            \"toxicity\": round(score, 4),\n",
    "            \"severity\": severity,\n",
    "            \"latencyinms\": latency\n",
    "        }\n",
    "\n",
    "# test run here\n",
    "if __name__ == \"__main__\":\n",
    "    bot = DetoxifySmart()\n",
    "    bot.load_and_train()\n",
    "    \n",
    "    # Test 1: Direct Toxicity\n",
    "    print(\"\\nTest 1:\", bot.filter_text({\"text\": \"You are so stupid and useless\", \"platform\": \"professional\"}))\n",
    "    \n",
    "    # Test 2: Slang/Context (TF-IDF usually fails here, Embeddings work)\n",
    "    print(\"\\nTest 2:\", bot.filter_text({\"text\": \"shut your mouth\", \"platform\": \"kids\"}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
